# Flexibel LLM-konfiguration f√∂r Email AI-kategorisering

## √ñversikt
Detta system st√∂der flera olika LLM-providers f√∂r email-kategorisering med automatisk fallback och regel-baserad backup.

## Konfiguration

### 1. Redigera `llm-config.json`

```json
{
  "providers": {
    "mistral": {
      "name": "Mistral 7B",
      "url": "http://localhost:1234",  // √Ñndra till din LLM-server
      "model": "mistral:7b",            // Modellnamn
      "endpoint": "/v1/chat/completions",
      "temperature": 0.4,
      "max_tokens": 250,
      "enabled": true,                  // Aktivera/avaktivera
      "priority": 1                     // L√§gre nummer = h√∂gre prioritet
    }
  }
}
```

### 2. Starta din LLM-server

F√∂r Mistral 7B med Ollama:
```bash
# Installera Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Ladda ner Mistral 7B
ollama pull mistral

# Starta server p√• port 11434 (standard)
ollama serve
```

F√∂r LM Studio eller annan OpenAI-kompatibel server:
```bash
# Starta LM Studio och ladda Mistral 7B
# S√§tt API-server till port 1234
```

### 3. Uppdatera konfiguration f√∂r din setup

√Ñndra i `llm-config.json`:

**F√∂r Ollama (port 11434):**
```json
"ollama": {
  "url": "http://localhost:11434",
  "model": "mistral",
  "endpoint": "/api/chat",
  "format": "ollama",
  "enabled": true,
  "priority": 1
}
```

**F√∂r LM Studio (port 1234):**
```json
"mistral": {
  "url": "http://localhost:1234",
  "model": "mistral-7b-instruct",
  "endpoint": "/v1/chat/completions",
  "enabled": true,
  "priority": 1
}
```

## Testning

### Testa LLM-konfiguration:
```bash
node test-llm-config.js
```

### Interaktivt l√§ge:
```bash
node test-llm-config.js --interactive
```

## Funktioner

### Automatisk Fallback
- F√∂rs√∂ker providers i prioritetsordning
- Om alla LLM:er misslyckas, anv√§nd regel-baserad klassificering
- Retry-logik med konfigurerbar delay

### Regel-baserad Backup
N√§r ingen LLM √§r tillg√§nglig anv√§nder systemet intelligent pattern-matching f√∂r att:
- Kategorisera emails (work/personal/newsletter/spam/promotional)
- Best√§mma prioritet (high/medium/low)
- Detektera sentiment (positive/neutral/negative)
- Identifiera action required

### St√∂d f√∂r flera LLM-format
- **OpenAI-kompatibel** (GPT-OSS, LM Studio, etc.)
- **Ollama** format
- **Custom providers** (l√§gg till i config)

## Integration med Email-service

### Anv√§nd i optimized-email-service.js:

```javascript
const FlexibleEmailAIAnalyzer = require('./flexible-ai-analyzer');

// Initiera med config
const aiAnalyzer = new FlexibleEmailAIAnalyzer('./llm-config.json');

// Anv√§nd f√∂r kategorisering
const result = await aiAnalyzer.classifyEmail(email);
```

### Milj√∂variabler (alternativ):
```bash
# √ñverstyr config med milj√∂variabler
LLM_URL=http://localhost:1234 \
LLM_MODEL=mistral:7b \
node optimized-email-service.js
```

## Exempel p√• providers

### Mistral 7B (rekommenderad f√∂r lokal k√∂rning)
- Bra balans mellan prestanda och resurser
- ~4GB RAM-krav
- Snabb f√∂r email-kategorisering

### Llama 2 7B
```json
{
  "url": "http://localhost:1234",
  "model": "llama2:7b",
  "temperature": 0.3
}
```

### GPT4All
```json
{
  "url": "http://localhost:4891",
  "model": "gpt4all-j",
  "temperature": 0.2
}
```

## Fels√∂kning

### LLM svarar inte:
1. Kontrollera att LLM-servern k√∂rs: `curl http://localhost:1234/v1/models`
2. Verifiera port och URL i config
3. K√∂r test: `node test-llm-config.js`

### D√•lig kategorisering:
1. Justera temperature (l√§gre = mer deterministisk)
2. √ñka max_tokens f√∂r l√§ngre svar
3. Byt till st√∂rre modell (13B eller 30B)

### F√∂r l√•ngsam respons:
1. Anv√§nd mindre modell (3B eller 7B)
2. Minska max_tokens
3. Aktivera GPU-acceleration om m√∂jligt

## Performance-tips

1. **Caching**: Systemet cachar redan kategoriseringar i Redis
2. **Batch-processing**: Anv√§nd batch-endpoints f√∂r flera emails
3. **Priority**: S√§tt snabbaste LLM:en som priority 1
4. **Fallback**: H√•ll regel-baserad som backup f√∂r 100% uptime

## Exempel-output

```
ü§ñ Trying Mistral 7B...
‚úÖ Success with Mistral 7B
Category: newsletter
Priority: low
Summary: Weekly tech newsletter with AI updates
Provider: Mistral 7B
```

N√§r LLM inte √§r tillg√§nglig:
```
‚ö†Ô∏è All LLM providers failed, using rule-based classification
Category: newsletter (rule-based)
Priority: low
Summary: Email about: Weekly Tech Newsletter
Provider: rule-based
```